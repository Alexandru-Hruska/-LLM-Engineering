{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e4e9f5-e75e-4461-98c3-3111117fa737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk\n",
      "Google API Key exists and begins AI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0510a17b-2277-4bb0-8200-6b6261e81c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()\n",
    "\n",
    "MODEL_OPTIONS = [\"gpt-3.5-turbo\", \"gpt-4\", \"ollama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc00ff33-024a-4a4a-83bc-8aa97f8c7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert Python developer and educator. Provide clear and concise explanations for the given code snippets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f13b5131-77a9-4d3a-b795-f6204850ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(model: str, question: str):\n",
    "    \"\"\"\n",
    "    Generates a streaming response from OpenAI's chat completions API.\n",
    "\n",
    "    Args:\n",
    "        model: The OpenAI model to use (e.g., \"gpt-3.5-turbo\", \"gpt-4\").\n",
    "        question: The user's query.\n",
    "\n",
    "    Yields:\n",
    "        The cumulative response string as it's generated.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    response = \"\"\n",
    "    for chunk in openai.chat.completions.create(model=model, messages=messages, stream=True):\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            response += content\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb06305-494a-4330-a9d7-193fa5a0d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ollama_response(question: str, model: str = OLLAMA_MODEL):\n",
    "    \"\"\"\n",
    "    Generates a streaming response from an Ollama model.\n",
    "\n",
    "    Args:\n",
    "        question: The user's query.\n",
    "        model: The Ollama model to use (default: OLLAMA_MODEL).\n",
    "\n",
    "    Yields:\n",
    "        The cumulative response string as it's generated.\n",
    "    \"\"\"\n",
    "    import ollama  # Import here to avoid errors if not using Ollama\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": question}], stream=True):\n",
    "        content = chunk['message']['content']\n",
    "        if content:\n",
    "            response += content\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec039bf5-39ef-42db-8093-547d00744074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Answer Function ---\n",
    "def answer_question(model: str, question: str):\n",
    "    \"\"\"\n",
    "    Routes the question to the appropriate LLM provider (OpenAI or Ollama)\n",
    "    and yields the streaming response to Gradio.\n",
    "\n",
    "    Args:\n",
    "        model: The selected LLM model.\n",
    "        question: The user's query.\n",
    "\n",
    "    Yields:\n",
    "        The streaming chunks of the response.\n",
    "    \"\"\"\n",
    "\n",
    "    generator = None\n",
    "\n",
    "    if model in [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\"]:\n",
    "        generator = get_openai_response(model, question)\n",
    "    elif model == \"ollama\":\n",
    "        try:\n",
    "            generator = get_ollama_response(question)\n",
    "        except ImportError:\n",
    "            yield \"Error: Ollama library not found. Please install it.\"\n",
    "            return\n",
    "        except Exception as e:\n",
    "            yield f\"Error with Ollama: {e}\"\n",
    "            return\n",
    "    else:\n",
    "        yield \"Selected model is not supported.\"\n",
    "        return\n",
    "\n",
    "    if generator:\n",
    "        for chunk in generator:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0bd0cdb-24fd-4b2c-bd32-4c51db4119a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7940\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7940/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Gradio Interface ---\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Technical Question Answerer\")\n",
    "\n",
    "    with gr.Row():\n",
    "        model_dropdown = gr.Dropdown(choices=MODEL_OPTIONS, label=\"Select Model\", value=MODEL_OPTIONS[0])\n",
    "    question_input = gr.Textbox(label=\"Enter your technical question here:\", lines=5)\n",
    "    answer_output = gr.Textbox(label=\"AI Explanation\", lines=10)\n",
    "    submit_button = gr.Button(\"Get Explanation\")\n",
    "\n",
    "    submit_button.click(fn=answer_question, inputs=[model_dropdown, question_input], outputs=[answer_output])\n",
    "\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc13d-c3cb-4c2e-8ee7-5c509a60b7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
