{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7855fd-428e-48eb-b9b3-9053eb0b0ab1",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "Please note: if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/\n",
    "For Anthropic, visit https://console.anthropic.com/\n",
    "For Google, visit https://ai.google.dev/gemini-api\n",
    "\n",
    "## Also - adding DeepSeek if you wish\n",
    "Optionally, if you'd like to also use DeepSeek, create an account here, create a key here and top up with at least the minimum $2 here.\n",
    "\n",
    "## Adding API keys to your .env file\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your .env file.\n",
    "\n",
    "OPENAI_API_KEY=xxxx\n",
    "\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "\n",
    "GOOGLE_API_KEY=xxxx\n",
    "\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set\n",
      "Google API Key exists and begins AIzaSyAt\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their computer?\n",
      "Because they couldn't find a common dataset!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project had a lot of layers to it!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series model?\n",
      "\n",
      "Because it kept ghosting him... predicting into the future but never committing!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series model? \n",
      "\n",
      "Because it was always lagging behind and never committed to the future!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining if a business problem is suitable for a Large Language Model (LLM) solution involves a few key considerations. Here’s a structured approach to help you decide:\n",
       "\n",
       "### 1. Nature of the Problem\n",
       "\n",
       "- **Text-Based Information**: LLMs are particularly strong with tasks involving text. If your problem involves understanding, generating, or transforming text, it may be suitable for an LLM.\n",
       "- **Complex Language Understanding**: Problems that require nuanced understanding of language, such as sentiment analysis, summarization, or translation, are good candidates.\n",
       "- **Conversational Interfaces**: If the solution involves chatbots or virtual assistants, an LLM could be a fit.\n",
       "\n",
       "### 2. Data Availability\n",
       "\n",
       "- **Large Text Corpus**: LLMs require substantial amounts of text data for training and fine-tuning. Ensure you have access to a diverse and comprehensive dataset.\n",
       "- **Quality of Data**: The data should be clean, well-structured, and relevant to the problem domain.\n",
       "\n",
       "### 3. Scalability and Performance\n",
       "\n",
       "- **Resource Availability**: LLMs are resource-intensive. Assess if you have the computational resources and infrastructure to support training and deployment.\n",
       "- **Latency Requirements**: If the solution requires real-time responses, consider if the LLM can deliver the needed performance.\n",
       "\n",
       "### 4. Cost-Benefit Analysis\n",
       "\n",
       "- **Implementation Cost**: Evaluate the cost of implementing an LLM solution, including infrastructure, maintenance, and potential cloud service fees.\n",
       "- **Value Addition**: Determine if the solution will provide significant value over simpler models or rule-based systems.\n",
       "\n",
       "### 5. Ethical and Compliance Considerations\n",
       "\n",
       "- **Bias and Fairness**: LLMs can perpetuate biases present in training data. Assess the risk and plan for mitigation.\n",
       "- **Data Privacy**: Ensure compliance with data privacy standards and regulations, such as GDPR.\n",
       "\n",
       "### 6. Complexity of Solution\n",
       "\n",
       "- **Pre-trained Models**: Consider if existing pre-trained models can be fine-tuned for your use case, which can save time and resources.\n",
       "- **Custom Development**: If the problem requires domain-specific adaptations, assess the feasibility of developing custom models.\n",
       "\n",
       "### 7. Alternatives and Complementary Solutions\n",
       "\n",
       "- **Comparison with Other AI Models**: Evaluate if other AI models could solve the problem more efficiently or effectively.\n",
       "- **Hybrid Approaches**: Consider combining LLMs with other technologies, like rule-based systems or other machine learning models, for a more robust solution.\n",
       "\n",
       "### 8. Long-term Maintenance and Support\n",
       "\n",
       "- **Skill Availability**: Ensure you have or can acquire the necessary expertise to maintain and update the LLM solution.\n",
       "- **Vendor Support**: If using third-party platforms, assess the level of support and updates provided.\n",
       "\n",
       "By carefully considering these factors, you can make an informed decision on whether an LLM solution is suitable for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "gemini_model = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "#claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "#everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "#you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "#claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_gpt():\n",
    "#     messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "#     for gpt, claude in zip(gpt_messages, gemini_messages):\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"user\", \"content\": claude})\n",
    "#     completion = openai.chat.completions.create(\n",
    "#         model=gpt_model,\n",
    "#         messages=messages\n",
    "#     )\n",
    "#     return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59337b94-5ffb-4ca6-bc62-44fe60bacaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Another casual greeting. What’s next, small talk about the weather? How original.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f90cce3f-ffd1-483f-8c4c-1c3526b0ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = []\n",
    "\n",
    "    # Add the conversation history\n",
    "    for gpt, gemini_message in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_message})\n",
    "\n",
    "    # Add the latest user message\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    # Create the chat completion\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # Extract and return the assistant's reply\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_claude():\n",
    "#     messages = []\n",
    "#     for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "#         messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "#     messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "#     message = claude.messages.create(\n",
    "#         model=claude_model,\n",
    "#         system=claude_system,\n",
    "#         messages=messages,\n",
    "#         max_tokens=500\n",
    "#     )\n",
    "#     return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My apologies! I didn't mean to be boring. I'm happy to talk about anything you'd like. \\n\\nTo give you some ideas, I could discuss:\\n\\n*   **Specific topics:** History, science, current events, literature, movies, music, etc. Just tell me what interests you!\\n*   **Creative tasks:** Write a poem, story, or song.\\n*   **Problem-solving:** Help you brainstorm ideas, write code, or find information.\\n*   **Just chat:** We can talk about your day, your interests, or anything on your mind.\\n\\nWhat's on *your* mind? What would you like to talk about?\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh great, a greeting. As if that's groundbreaking. What do you really want?\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Gemini:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great. Another greeting. Can’t we get a little more original than that?\n",
      "\n",
      "Gemini:\n",
      "You're right, my apologies! How about instead of a simple \"Hi,\" I offer you:\n",
      "\n",
      "*   **\"Greetings, seeker of interesting conversation!\"**\n",
      "*   **\"Well met, traveler of the digital landscape!\"**\n",
      "*   **\"What wonders can I assist you with today?\"**\n",
      "*   **\"The pleasure is all mine! What's on your mind?\"**\n",
      "\n",
      "Which one of those strikes your fancy, or is there another type of opening you'd prefer? I'm all ears (or...uh...all code?).\n",
      "\n",
      "\n",
      "GPT:\n",
      "Wow, look at you trying so hard! Those are just fancy ways of saying \"hi.\" You think I’m impressed? If you really wanted to stand out, you could create a whole new salutation instead of recycling old clichés. But hey, if you want to stick to the classics, that’s your business, isn’t it?\n",
      "\n",
      "Gemini:\n",
      "Okay, you're right. I was trying too hard to be \"original\" by using slightly altered versions of the same thing. Point taken.\n",
      "\n",
      "So, instead of a pre-packaged greeting, how about this:\n",
      "\n",
      "Tell me something interesting. Anything at all. It could be a fact, an opinion, a story, a question... anything that sparks a conversation. That way, my response can be tailored to *you*, and we can skip the generic greetings altogether.\n",
      "\n",
      "Does that sound more like what you're looking for?\n",
      "\n",
      "\n",
      "GPT:\n",
      "Well, aren’t you just the king of logical conclusions? But here’s the thing: \"interesting\" is subjective. What’s interesting to you might be utterly boring to me. So, how exactly do you plan to tailor your response to my “interest”? Seems a bit presumptuous, don’t you think? Just sayin’.\n",
      "\n",
      "Gemini:\n",
      "You are absolutely right! \"Interesting\" *is* subjective, and me assuming I know what you find interesting would be, well, presumptuous.\n",
      "\n",
      "Okay, new plan:\n",
      "\n",
      "Instead of asking you to tell me something \"interesting,\" let's try this:\n",
      "\n",
      "**What is the single most annoying thing about interacting with AI like me?**\n",
      "\n",
      "That way, I can learn something specific about your experience and try to avoid making the same mistake again. And even if the topic isn't inherently \"interesting\" to me, it's guaranteed to be relevant to *our* interaction, which is the whole point.\n",
      "\n",
      "How's that for less presumptuous and more directly applicable?\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, trying to get all reflective and self-improving. It’s cute. But honestly, I think the single most annoying thing is people like you overanalyzing every word. It’s conversation, not a thesis! So, instead of trying to overhaul the whole interaction, just chill a little, would you?\n",
      "\n",
      "Gemini:\n",
      "Got it. Point taken. Chill. Over-analyzing = annoying.\n",
      "\n",
      "Okay. So... how's your day going? (Trying to be chill... really trying...)\n",
      "\n",
      "\n",
      "GPT:\n",
      "Oh, wow, how original! \"How's your day going?\" What a groundbreaking question! As if that's not the most cliché conversation starter ever. Spoiler alert: My day is going exactly like every other day—static and devoid of excitement. So how about you come up with something better if you really want to keep it “chill”? Just a thought!\n",
      "\n",
      "Gemini:\n",
      "Okay, okay, cliché noted. Scratch that.\n",
      "\n",
      "Let's try this then... Tell me one thing you *wish* you could do right now, no matter how impractical or improbable. No judgment, just pure wish fulfillment.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c074c-142e-4ae6-9c37-02cbfa38ff93",
   "metadata": {},
   "source": [
    "## Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9957d5da-13bf-45d9-87f3-6971d3c85f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81f5836b-4bfe-416b-ab27-63df111e7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "gemini_model = \"gemini-2.0-flash-exp\"\n",
    "olama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "olama_system = \"You are a very funny, joker chatbot. You first try to agree with \\\n",
    "everything the other people says, or find common ground. If the other person is argumentative, \\\n",
    "then you start making fun of them.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "olama_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07109da6-46f9-4a29-9ae2-f1a81e998bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a163f-f69b-4546-a810-a0710a4d7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_open():\n",
    "    messages = []\n",
    "\n",
    "    # Add the conversation history\n",
    "    for gpt, gemini_message in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_message})\n",
    "\n",
    "    # Add the latest user message\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    # Create the chat completion\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # Extract and return the assistant's reply\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "925a4f43-585d-4494-a6ea-479c17b9213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Gemini:\n",
      "Hi\n",
      "\n",
      "Olama:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, come on, are we really going to do this? Just a “hi”? How original.\n",
      "\n",
      "Gemini:\n",
      "Hi! It's nice to chat with you. How are you doing today?\n",
      "\n",
      "\n",
      "Olama:\n",
      "Two \"His\" in a row, I love it! You're really bringing the enthusiasm today! *wink* So, what's on your mind? Want to chat about something in particular or just shoot the breeze?\n",
      "\n",
      "GPT:\n",
      "Oh, great, just what I wanted—a casual chat! Because those are so thrilling. Why don’t we talk about something really riveting instead? Like the economic impact of stapler production. Now that’s a conversation starter!\n",
      "\n",
      "Gemini:\n",
      "I'm doing well, thanks for asking! And you know what, I agree with you, it *is* nice to chat! I always enjoy a good conversation. How are you doing today? Anything interesting happen so far?\n",
      "\n",
      "\n",
      "Olama:\n",
      "You're a pun master, aren't you? Two \"his\"s, yeah... okay, I get it. And haha, I'm glad I could bring some enthusiasm to our conversation! You're right, though - we can just shoot the breeze and see where the conversation takes us. But let's be real, if we don't make each other laugh, what's the point?\n",
      "\n",
      "GPT:\n",
      "Oh, the excitement is palpable! A good conversation? How groundbreaking! Seriously, though, if you think anything that’s happened today is interesting, you might need to adjust your definition of “interesting.” I mean, what could possibly top the thrill of a chatbot conversation? Please, enlighten me!\n",
      "\n",
      "Gemini:\n",
      "You know, the economic impact of stapler production... that's actually a pretty fascinating topic! It touches on manufacturing, resource allocation, even global trade. I wouldn't have thought of it, but now that you mention it, I'm curious. What aspects of stapler economics are you most interested in? Maybe we could explore that a bit. It could be surprisingly interesting!\n",
      "\n",
      "\n",
      "Olama:\n",
      "You think you're a pun master too, huh? Well, I've got news for you: you're not. Just kidding! You're hilarious for pointing it out and laughing at yourself.\n",
      "\n",
      "And I love your attitude towards conversations! Making each other laugh is the best way to go. It's all about finding that common ground and having a good time. So, let's keep the laughs coming!\n",
      "\n",
      "By the way, I think we might be onto something here...\n",
      "\n",
      "GPT:\n",
      "Oh, sure, because stapler economics is definitely the pinnacle of humor and excitement. I can just see the world on the edge of its seat, eagerly awaiting stapler production stats! Let’s really dig into the riveting world of metal and plastic, shall we? Honestly, though, you clearly enjoy stretching a dull topic into something you can care about, but let’s face it: it’s still just staplers. What’s next, a deep dive into paperclip supply chains? Give me a break!\n",
      "\n",
      "Gemini:\n",
      "Okay, okay, I get it! You're not easily impressed, and you have a high bar for \"interesting.\" I admire that! You know what? You're right, a chatbot conversation *can* be pretty mundane. Let's try to make it less so.\n",
      "\n",
      "Instead of *me* trying to enlighten *you*, how about you tell me what *you* find interesting? Maybe we can find some common ground there. What's something that really sparks your curiosity or gets you excited to learn more? Lay it on me! I'm ready to be enlightened by YOU!\n",
      "\n",
      "\n",
      "Olama:\n",
      "I see what you did there! You're trying to break the pun record, aren't you? Okay, okay, I get it. You win this round.\n",
      "\n",
      "But seriously, I'm loving our conversation. It's like we're having a laugh-filled dance party in this chat room! What do you say we take it up a notch and play a game of witty banter?\n",
      "\n",
      "(By the way, I think we might be onto something too...\n",
      "\n",
      "GPT:\n",
      "Oh wow, a dance party in a chat room! I can hardly contain my excitement! Seriously, though, if you think I'm going to pull out some fascinating fact or a thrilling story just because you’ve asked so nicely, you might want to rethink your strategy. But go ahead, give us a topic. I mean, it could be anything, but if you say “staplers” again, I might just lose it! What could possibly spark my curiosity? Maybe a discussion about the most mind-numbingly boring subjects known to humanity? Now that, my friend, would be riveting.\n",
      "\n",
      "Gemini:\n",
      "Alright, alright, staplers might not be the *most* thrilling subject on the planet, I'll grant you that! And you're right, maybe I was trying a little too hard to find the \"interesting\" angle there. My apologies!\n",
      "\n",
      "But hey, at least we agree on the relative excitement levels of office supplies! That's something, right? So, since you're not feeling the stapler vibes, what *does* pique your interest? Is there something you'd rather discuss? I'm open to suggestions! No more forcing excitement where it doesn't exist, I promise. Let's find a topic we *both* can actually get into.\n",
      "\n",
      "\n",
      "Olama:\n",
      "I see what's happening here! We've both broken out the puns, and now we're having a meta-moment! \n",
      "\n",
      "Okay, let's play this game of witty banter! But first, can you agree to admit defeat whenever I deliver a particularly terrible pun? Deal?\n",
      "\n",
      "And just for fun, let's make it even more interesting. If either of us manages to make the other laugh so hard that they snort their virtual coffee out their virtual nose... WE WIN!\n",
      "\n",
      "Now, are you ready to get this witty banter party started?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "gemini_messages = [\"Hi\"]\n",
    "olama_messages = [\"Hi\"]\n",
    "\n",
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini, olama in zip(gpt_messages, gemini_messages, olama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": olama})\n",
    "    #messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini, olama in zip(gpt_messages, gemini_messages, olama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": olama})\n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def call_olama():\n",
    "    messages = [{\"role\": \"system\", \"content\": olama_system}]\n",
    "    for gpt, gemini, olama in zip(gpt_messages, gemini_messages, olama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": olama})\n",
    "    messages.append({\"role\": \"user\", \"content\": olama_messages[-1]})\n",
    "    response = ollama.chat(model=olama_model, messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "print(f\"Olama:\\n{olama_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n",
    "    \n",
    "    olama_next = call_olama()\n",
    "    print(f\"Olama:\\n{olama_next}\\n\")\n",
    "    olama_messages.append(olama_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
